# pipeline_cmd_vel.py

import cv2
import numpy as np
import pyrealsense2 as rs
import signal, time
from ultralytics import YOLO
import math

import torch
from torchvision import models, transforms
import cv2, numpy as np

# ì¶”ê°€
#from groundingdino.util.inference import load_model, predict
import torchvision.transforms as T
import patient_info as info


# --- ROS2 ì¶”ê°€ ---
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist
from rclpy.qos import QoSProfile, ReliabilityPolicy



# FPS ì¸¡ì •ìš©
from collections import deque
import time
fps_history = deque(maxlen=10)


# ===============================================================
#                  ROS2 Node ì •ì˜
# ===============================================================
class CmdVelPublisher(Node):
    def __init__(self):
        super().__init__('gown_apf_publisher')
        qos = QoSProfile(depth=10, reliability=ReliabilityPolicy.RELIABLE)
        self.publisher_ = self.create_publisher(Twist, '/cmd_vel', qos)

    def publish_cmd(self, linear, angular):
        msg = Twist()
        msg.linear.x = float(max(min(linear, 0.5), -0.5))   # ì œí•œ
        msg.angular.z = float(max(min(angular, 1.0), -1.0))
        self.publisher_.publish(msg)
        self.get_logger().info(
            f"ğŸ“¤ /cmd_vel -> linear.x={msg.linear.x:.3f}, angular.z={msg.angular.z:.3f}"
        )




# ì „ì²˜ë¦¬ ì •ì˜
transform = transforms.Compose([
    transforms.ToPILImage(),  
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])


# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
g_model = models.mobilenet_v2(pretrained=False) 
g_model.classifier[1] = torch.nn.Linear(g_model.last_channel, 2)
g_model.load_state_dict(torch.load("gown_classifier.pth", map_location="cpu"))
g_model.eval()




# ====== ì„¤ì • ======
WIN = "Patient-Gown + Marker (ESC/q)"
YOLO_WEIGHTS = "yolov8n.pt"     # n/s/m ë¡œ êµì²´ ê°€ëŠ¥ yolov8s.pt(Small), yolov8m.pt(Medium)
PERSON_CONF = 0.5       # ìµœì†Œ ì‹ ë¢°ë„(confidence) ì„ê³„ê°’.
COLOR_RES = (640, 480)  # (1280, 720) (640, 480)
FPS = 60
USE_DEPTH = True        # ê±°ë¦¬ ì¶”ì • ì›í•˜ë©´ True
ARUCO_DICT = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
ARUCO_PARAMS = cv2.aruco.DetectorParameters()
# ==================

stop = False
def _sigint(sig, frame):
    global stop; stop = True
signal.signal(signal.SIGINT, _sigint)

# ---------- í™˜ìë³µ íŒë³„ ----------MobileNetV2
def is_patient_gown(crop_bgr: np.ndarray) -> bool:
    if crop_bgr.size == 0:
        return False
    img = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (224,224))
    tensor = transform(img).unsqueeze(0)  # (1,3,224,224)
    img_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
    x = transform(img_rgb).unsqueeze(0)  # transform ì•ˆì—ì„œ ToPILImage()ê°€ PIL ë³€í™˜+Resize ìˆ˜í–‰

    
    with torch.no_grad():
        out = g_model(tensor)
        pred = torch.argmax(out, 1).item()
    return pred == 0  # 0=gown, 1=normal (ImageFolder ìˆœì„œ ê¸°ì¤€) 


# ---------- QR/ë°”ì½”ë“œ + QR(OpenCV) ----------
try:
    from pyzbar import pyzbar
    HAVE_PYZBAR = True
except Exception:
    HAVE_PYZBAR = False
qrd = cv2.QRCodeDetector()

def decode_markers(bgr):
    """
    ROIì—ì„œ QR/ë°”ì½”ë“œ/ArUco íƒì§€.
    ë°˜í™˜: dict { 'qr':[(txt, pts)], 'aruco':[(id, corners)] }
    pts/corners ëŠ” ROI ì¢Œí‘œê³„ ê¸°ì¤€ np.int32
    """
    out = {'qr': [], 'aruco': []}

    # 1) QR/ë°”ì½”ë“œ: pyzbar ìš°ì„ 
    if HAVE_PYZBAR:
        try:
            for c in pyzbar.decode(bgr):
                txt = c.data.decode('utf-8', errors='ignore')
                pts = np.array(c.polygon, dtype=np.int32).reshape(-1, 2) if c.polygon else None
                out['qr'].append((txt, pts))
        except Exception:
            pass

    # 2) OpenCV QR í´ë°±/ë³´ê°•
    try:
        ok, texts, points, _ = qrd.detectAndDecodeMulti(bgr)
        if ok and texts is not None:
            for i, t in enumerate(texts):
                if t:
                    pts = points[i].astype(int) if points is not None else None
                    out['qr'].append((t, pts))
        else:
            t, pts, _ = qrd.detectAndDecode(bgr)
            if t:
                pts = pts.astype(int) if pts is not None else None
                out['qr'].append((t, pts))
    except Exception:
        pass

    # 3) ArUco
    try:
        detector = cv2.aruco.ArucoDetector(ARUCO_DICT, ARUCO_PARAMS)
        corners, ids, _ = detector.detectMarkers(bgr)
        if ids is not None:
            for i, cid in enumerate(ids.flatten()):
                cs = corners[i].astype(int).reshape(-1, 2)
                out['aruco'].append((int(cid), cs))
    except Exception:
        pass

    return out

# ---------- ìœ í‹¸ ----------
def clamp_box(x1,y1,x2,y2, W,H):
    return max(0,x1), max(0,y1), min(W,x2), min(H,y2)

def draw_poly(img, pts, color=(0,255,0), thickness=2):
    if pts is not None and len(pts) >= 4:
        cv2.polylines(img, [pts], True, color, thickness)


#============ê°ë„ ê³„ì‚°, x, y ê±°ë¦¬ ê³„ì‚°==========

def calculate_theta(cxy_x : int) -> str:
    frame_width = 640 # 640ì¤‘ ì¤‘ê°„ í”½ì…€
    fov_deg = 69.4
    fov_rad = math.radians(fov_deg)

    # ì¤‘ì‹¬ ëŒ€ë¹„ ìƒëŒ€ ìœ„ì¹˜ ë¹„ìœ¨ (-1.0 ~ +1.0)
    rel = (cxy_x-frame_width/2) / (frame_width / 2)

    # ì¢Œìš° ê°ë„ (radian)
    theta_rad = rel * (fov_rad / 2)
    return theta_rad
#  # ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜


def calculate_vector(cxy_x, real_dist):
    theta_rad = calculate_theta(cxy_x)
    # real_distì˜ x, yì„±ë¶„
    dx = math.cos(theta_rad)*real_dist
    dy = math.sin(theta_rad)*real_dist
    # 1mì˜ x, yì„±ë¶„
    dx_1m = math.cos(theta_rad)*1.00
    dy_1m = math.sin(theta_rad)*1.00
    return dx, dy, dx_1m, dy_1m

def Artificial_Potention_Field(cxy_x, real_dist, k_att=3.0, stop_dist=1.0):
    # ê·¼ë° ì´ê±° ì¼ë‹¨ attractive forceë§Œ, ì•„ì§ replusive ëŠ” êµ¬í˜„ ì•ˆí•¨
    dx, dy, dx_1m, dy_1m = calculate_vector(cxy_x, real_dist)
    dist = math.hypot(dx, dy)
    delta = dist - stop_dist
    # Attractive force ê³„ì‚°
    if delta <= 0:
        return 0.0, 0.0  # ì¼ì • ê±°ë¦¬ ì´ë‚´ë©´ ë©ˆì¶¤
    
    apf_dist = k_att*delta
    theta = calculate_theta(cxy_x)
    apf_delta = k_att*theta

    apf_dist = k_att*delta
    theta = calculate_theta(cxy_x)
    theta_p = math.pow(abs(theta), 1.5)*(theta/abs(theta))
    apf_delta = k_att*theta_p
    return apf_dist, apf_delta


def get_apf_inputs(cxy_x, real_dist_m):
    return cxy_x, real_dist_m


def main():
    global stop

    rclpy.init()                        # --- ROS2 INIT ---
    node = CmdVelPublisher()            # --- ROS2 Node ìƒì„± ---

    # YOLO ë¡œë“œ + ì›Œë°ì—…
    model = YOLO(YOLO_WEIGHTS)
    _ = model.predict(np.zeros((480,640,3), dtype=np.uint8), verbose=False)

    # ==== GroundingDINO ë¡œë“œ ====
    # GROUNDDINO_CONFIG = "../../../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py"
    # GROUNDDINO_WEIGHTS = "../../../GroundingDINO/weights/groundingdino_swint_ogc.pth"
    # g_dino_model = load_model(GROUNDDINO_CONFIG, GROUNDDINO_WEIGHTS)
    # device = next(g_dino_model.parameters()).device


    # RealSense íŒŒì´í”„ë¼ì¸
    pipe = rs.pipeline()
    cfg = rs.config()
    cfg.enable_stream(rs.stream.color, COLOR_RES[0], COLOR_RES[1], rs.format.bgr8, FPS)
    if USE_DEPTH:
        cfg.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, FPS)
        align = rs.align(rs.stream.color)
    profile = pipe.start(cfg)

    cv2.namedWindow(WIN, cv2.WINDOW_NORMAL)

    try:
        while not stop:
            # ğŸ”¹ FPS ì‹œì‘ ì‹œê°
            start_time = time.time()



            if cv2.getWindowProperty(WIN, cv2.WND_PROP_VISIBLE) < 1:
                break

            frames = pipe.wait_for_frames()
            if USE_DEPTH:
                frames = align.process(frames)

            color_f = frames.get_color_frame()
            if not color_f:
                if (cv2.waitKey(1) & 0xFF) in (27, ord('q')): break
                continue
            img = np.asanyarray(color_f.get_data())
            H, W = img.shape[:2]

            depth_f = frames.get_depth_frame() if USE_DEPTH else None

            # 1) ì‚¬ëŒ íƒì§€



            
            res = model.predict(img, classes=[0], conf=PERSON_CONF, verbose=False)
            for r in res:
                for b in r.boxes:
                    x1, y1, x2, y2 = map(int, b.xyxy[0].tolist())
                    x1,y1,x2,y2 = clamp_box(x1,y1,x2,y2, W,H)
                    cv2.rectangle(img, (x1,y1),(x2,y2), (255,0,0), 2)
            # === GroundingDINO (ëŒ€ì²´) ===
            
            
            # # BGR â†’ RGB â†’ Tensor ë³€í™˜
            # rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            # tensor_img = T.ToTensor()(rgb_img).to(device)



            # # ì‚¬ëŒ íƒì§€ ìˆ˜í–‰
            # boxes, logits, phrases = predict(
            #     model=g_dino_model,
            #     image=tensor_img,
                
            #     caption="a person who is standing",
            #     box_threshold=0.35,
            #     text_threshold=0.30
            # )


            
            #print(f"[DEBUG] Detected boxes: {len(boxes)}")

            # for box in boxes:
            #     # GroundingDINOëŠ” (cx, cy, w, h)ì¼ ìˆ˜ ìˆìŒ â†’ ë³€í™˜
            #     if len(box) == 4:
            #         cx, cy, w, h = box.tolist()
            #         x1 = (cx - w / 2)
            #         y1 = (cy - h / 2)
            #         x2 = (cx + w / 2)
            #         y2 = (cy + h / 2)
            #     else:
            #         x1, y1, x2, y2 = box.tolist()

                # # ì •ê·œí™” ì¢Œí‘œë©´ í”½ì…€ ë‹¨ìœ„ë¡œ ë³€í™˜
                # if 0 <= x2 <= 1 and 0 <= y2 <= 1:
                #     x1, x2 = x1 * W, x2 * W
                #     y1, y2 = y1 * H, y2 * H

                # # ì •ë ¬ ë° í´ë¨í”„
                # x1, x2 = sorted([int(x1), int(x2)])
                # y1, y2 = sorted([int(y1), int(y2)])
                # x1, y1 = max(0, x1), max(0, y1)
                # x2, y2 = min(W, x2), min(H, y2)

                # if (x2 - x1) < 10 or (y2 - y1) < 10:
                #     continue

                # print(f"[DEBUG] Corrected box: ({x1},{y1})-({x2},{y2})")
                # cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)





                    # 2) ìƒì²´ ROI (í™˜ìë³µ íŒë³„ìš©)
                    ph = y2 - y1
                    torso_y2 = y1 + int(ph*0.65)
                    tx1, ty1, tx2, ty2 = x1, y1, x2, max(y1+1, torso_y2)
                    torso = img[ty1:ty2, tx1:tx2]
                    gown = is_patient_gown(torso)

                    label = f"person ({'gown' if gown else 'clothes'})"
                    cv2.putText(img, label, (x1, y1-6), cv2.FONT_HERSHEY_SIMPLEX, 0.40,
                                (0,255,0) if gown else (0,165,255), 2)

                    if not gown:
                        continue  # í™˜ìë³µ ì•„ë‹ˆë©´ ë„˜ì–´ê° (ì •ì±…ìƒ í•„ìš” ì‹œ ì œê±°)

                    # 3) í™˜ìë³µìœ¼ë¡œ íŒì •ëœ ê²½ìš°: ì‚¬ëŒ ROI ì „ì²´ì—ì„œ ë§ˆì»¤ íƒì§€
                    roi = img[y1:y2, x1:x2]
                    # ì‘ì€ ROIëŠ” ì—…ìƒ˜í”Œ
                    scale_up = 1.0
                    if max(roi.shape[:2]) < 420:
                        roi = cv2.resize(roi, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_NEAREST)
                        scale_up = 1.5

                    det = decode_markers(roi)

                    # # 3-1) QR ê²°ê³¼
                    # for txt, pts in det['qr']:
                    #     # ì¤‘ì‹¬ ê³„ì‚°
                    #     if pts is not None and len(pts) >= 4:
                    #         cxy = pts.mean(axis=0).astype(int)
                    #         # ì›ë³¸ ì¢Œí‘œë¡œ ë³´ì •
                    #         cp = (int(x1 + cxy[0]/scale_up), int(y1 + cxy[1]/scale_up))
                    #         cv2.circle(img, cp, 4, (0,255,0), -1)
                    #         draw_poly(img, (pts/scale_up).astype(int) + np.array([x1,y1]))
                    #     else:
                    #         # í´ë¦¬ê³¤ì´ ì—†ìœ¼ë©´ ROI ì¤‘ì•™
                    #         cp = (int((x1+x2)//2), int((y1+y2)//2))
                    #     # ê±°ë¦¬ ì¶”ì •
                    #     dist_str = ""
                    #     if USE_DEPTH and depth_f:
                    #         d = depth_f.get_distance(cp[0], cp[1])
                    #         if d > 0:
                    #             dist_str = f" | {d:.2f}m"
                    #     cv2.putText(img, f"QR:{txt}{dist_str}", (x1+5, max(20,y1-10)),
                    #                 cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0,255,0), 2)

                    # 3-2) ArUco ê²°ê³¼
                    for mid, corners in det['aruco']:
                        pts = (corners/scale_up).astype(int) + np.array([x1,y1])
                        draw_poly(img, pts, (0,255,255), 2)
                        # ì¤‘ì‹¬
                        cxy = pts.mean(axis=0).astype(int)
                        dist_str = ""
                        if USE_DEPTH and depth_f:
                            d = depth_f.get_distance(int(cxy[0]), int(cxy[1]))
                            if d > 0: dist_str = f" | {d:.2f}m"
                        cv2.putText(img, f"ArUco : {mid}{dist_str}", (pts[0,0]+30, pts[0,1]-6),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0,255,255), 2)
                        
                        cv2.circle(img, tuple(cxy), 4, (0,255,255), -1)


                        #is it ok???????????
                        depth = d * 100
                        real_dist = info.calculate_range(str(mid), depth)
                            # ì•ˆì „ í¬ë§· ì²˜ë¦¬
                        if real_dist is None:
                            dist_text = "REAL_DISTANCE = N/A"
                        else:
                            dist_text = f"REAL_DISTANCE = {real_dist:.2f} cm"

                        cv2.putText(img, dist_text, (pts[0,0]+30, pts[0,1]+20),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,255,255), 2)
                        
                        # ê°ë„ ê³„ì‚° printinf
                        theta = calculate_theta(cxy[0]) 
                        theta_s = f"{theta:.2f} rad"  # ë¬¸ìì—´ë¡œ ë³€í™˜
                        cv2.putText(img, theta_s, (pts[0,0]+30, pts[0,1]+40),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,255,255), 2)
                                            
    

                        # --- ì¸ê³µí¼í…ì…œí•„ë“œ(APF) ê³„ì‚° ê²°ê³¼ í‘œì‹œ ---
                        if real_dist is not None:
                            real_dist_m = real_dist / 100.0  # cm â†’ m ë³€í™˜
                            v, theta_rad = Artificial_Potention_Field(cxy[0], real_dist_m)
                            
                            # ğŸŸ¢ ROS í¼ë¸”ë¦¬ì‹œ ì¶”ê°€
                            node.publish_cmd(v, theta_rad)
                            # ì†ë„ì™€ ê°ë„ ê²°ê³¼ ë¬¸ìì—´ ìƒì„±
                            apf_text = f"APF -> v: {v:.2f}, theta: {math.degrees(theta_rad):.2f}Â°"

                            # í™”ë©´ í‘œì‹œ
                            cv2.putText(img, apf_text, (pts[0,0]+30, pts[0,1]+60),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0,255,255), 2)


                        
                        # myname
                        patient_data = info.get_patient_info(str(mid))

                        if not patient_data or not isinstance(patient_data, dict):
                            myname = "Unknown"
                        else:
                            myname = patient_data.get("final_name", "Unknown")
                        cv2.putText(img, f"Name : {myname}", (pts[0,0]+30, pts[0,1]-26),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0,255,0), 2)

                                
            # ğŸ”¹ FPS ê³„ì‚° ë° í‘œì‹œ (imshow ì§ì „)
            end_time = time.time()
            frame_time = end_time - start_time
            if frame_time > 0:
                fps_history.append(1.0 / frame_time)
                avg_fps = sum(fps_history) / len(fps_history)
            else:
                avg_fps = 0.0

            cv2.putText(img, f"FPS: {avg_fps:.2f}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)

            
                        

            cv2.imshow(WIN, img)
            k = cv2.waitKey(1) & 0xFF
            if k in (27, ord('q')):
                break
            rclpy.spin_once(node, timeout_sec=0)  # ROS ì´ë²¤íŠ¸ ì²˜ë¦¬

    finally:
        pipe.stop()
        rclpy.shutdown()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()

